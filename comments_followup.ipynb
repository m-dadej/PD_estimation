{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up on comments during project presentation\n",
    "\n",
    "Herein document adresses some issues/comments that were mentioned during project presentation. \n",
    "The document is written in Jupyter notebook as some of the issues were regarding the exact calculations and models performance.\n",
    "\n",
    "The few code chunks below reads data and import some code in order to investigate the issues more in depth. The code is ran with the RNG same seed as `analysis.ipynb`, so the results are comparable to the ones shown in slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import random as rnd\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: orbis_active_be.xlsx ...\n",
      "Reading file: orbis_active_de.xlsx ...\n",
      "Reading file: orbis_active_dk.xlsx ...\n",
      "Reading file: orbis_active_es.xlsx ...\n",
      "Reading file: orbis_active_fin.xlsx ...\n",
      "Reading file: orbis_active_fra.xlsx ...\n",
      "Reading file: orbis_active_it.xlsx ...\n",
      "Reading file: orbis_active_no.xlsx ...\n",
      "Reading file: orbis_active_rest.xlsx ...\n",
      "Reading file: orbis_active_se.xlsx ...\n",
      "Reading file: orbis_default.xlsx ...\n"
     ]
    }
   ],
   "source": [
    "%run -i src/data_preproc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_share = 0.8\n",
    "\n",
    "rnd.seed(1)\n",
    "train_id = rnd.sample(range(df.shape[0]), round(df.shape[0] * split_share))\n",
    "\n",
    "train_df = df.drop(['country', 'last_year', 'sector'], axis=1).loc[train_id]\n",
    "test_df  = df.drop(['country', 'last_year', 'sector'], axis=1).loc[~np.isin(list(range(df.shape[0])), train_id)]\n",
    "\n",
    "for variable in train_df.loc[:, train_df.apply(lambda x: any(x.isna()))].columns:\n",
    "    train_df.loc[train_df[variable].isna(), variable] = train_df[variable].median()\n",
    "    test_df.loc[test_df[variable].isna(), variable] = test_df[variable].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume the problem is persistent among every model, so for simplicity I will analyze only XGBoost trained on undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_us = joblib.load('models/xgb_us.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"src/pred_metrics_class.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low F1 score of the models\n",
    "\n",
    "As table below shows, the F1 score of the models appears to be very low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](latex/img/balance_acc_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code I used the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives the same result as the sklearn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_xgb_us = PredMetrics(pred_pd = xgb_us.predict_proba(np.array(test_df.loc[:,test_df.columns != 'Inactive']))[:,1],\n",
    "                             actual = np.array(test_df.Inactive))\n",
    "\n",
    "metrics_xgb_us.f1_score(0.5) == metrics_xgb_us.f1_score_sk(0.5, 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrapped the sklearn function into the class for code simplicity. You can check it in the `pred_metrics_class.py`\n",
    "\n",
    "Since the $F_1$ score is a function of recall and precision we can check both of these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall: ': 0.8071748878923767, 'precision: ': 0.07100124909604891}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'recall: ': metrics_xgb_us.tpr(0.5),\n",
    " 'precision: ': metrics_xgb_us.ppv(0.5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model has a problem with precision, i.e. when model predicts a default, there is actually low chance that the corporate will default. \n",
    "Before I will check it further, I think it is worth to note, that this kind of trade off combination between precision and recall is better than the other way around as it's usually less painful not to give loan (low precision means the model very often was wrong while predicting default, so not giving loan was very often wrong decision) unlike giving a loan to the future default (high recall means, that when we identified high share of defaults, thus the loans were safe).\n",
    "\n",
    "Additionally, despite some sources (e.g. [this](https://deepai.org/machine-learning-glossary-and-terms/f-score) article on the first page after searching \"F1 score\" in google) claiming that its not sensitive to the class imbalance, it indeed is. And it's the case in my application.\n",
    "\n",
    "Let's consider a random prediction of imbalanced and balanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced :': 0.499, '/n imbalanced :': 0.168}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_metric_balance = PredMetrics(pred_pd = np.random.choice([0,1], 10000, p = [0.5, 0.5]),\n",
    "                                    actual = np.random.choice([0,1], 10000, p = [0.5, 0.5]))\n",
    "\n",
    "random_metric_imbalance = PredMetrics(pred_pd = np.random.choice([0,1], 10000, p = [0.5, 0.5]),\n",
    "                                      actual = np.random.choice([0,1], 10000, p = [0.9, 0.1]))\n",
    "\n",
    "{'balanced :': round(random_metric_balance.f1_score(0.5), 3),\n",
    " 'imbalanced :': round(random_metric_imbalance.f1_score(0.5), 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, despite both of the 'models' having the same predicting ability, their F1 score varies. The F1 score varies also when both of the predictions are imbalanced (i.e. when models were trained on rebalanced dataset).\n",
    "\n",
    "Given this properity we can compare actual models to the benchmark 'guessing' models with the same class imbalance of test dataset and prediction values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random prediction :': 0.04769569847205369,\n",
       " 'xgb_us prediction :': 0.1305214816605233}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_metrics = PredMetrics(pred_pd = np.random.choice([0,1], \n",
    "                                                        test_df.Inactive.shape[0], \n",
    "                                                        p = [sum(metrics_xgb_us.df_compare.default_prob >= 0.5) / metrics_xgb_us.df_compare.shape[0], \n",
    "                                                             sum(metrics_xgb_us.df_compare.default_prob < 0.5) / metrics_xgb_us.df_compare.shape[0]]),\n",
    "                             actual = np.array(test_df.Inactive))\n",
    "\n",
    "{'random prediction :': random_metrics.f1_score(0.5),\n",
    " 'xgb_us prediction :': metrics_xgb_us.f1_score(0.5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the context above, the model F1 score turns out to be very good (for the level of imbalance of the test dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct logistic regression with $L_1$ and $L_2$ regularization (elastic net)\n",
    "\n",
    "I have corrected the previous formula in the slides to (as per Hastie et al ESL):\n",
    "\n",
    "$$\\min_{\\beta} \\alpha \\beta^2 + (1-\\alpha)|\\beta| - C \\sum_{i = 1}^n y_i \\beta ' X_i - \\ln 1 + e^{\\beta'X_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the impact of multicollinearity on the model, I've removed variables that with high correlation (on condition that $|R^2| > 0.5$). \n",
    "\n",
    "Table below shows the out-of-sample performance of the models with a 50% threshold (the same as calculated for original project):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](latex/img/balance_acc_table_no_multicor.png)\n",
    "\n",
    "And below is the table of performance for models with multicolinearity:\n",
    "\n",
    "![title](latex/img/balance_acc_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is rather small for the overall performance of models (balanced accuracy), the most significant impact is on the structure of the errors. The more complex ML models (xgb and mlpc) were better at correcting prositives and slightly worse at predicting negatives. Apart from the positive impact on general performance, this trade-off is more favorable.\n",
    "\n",
    "Given the slight but positive impact on the perfromance and a lessen complexity (because of higher sparsity of models), the variable redction would be indeed a good choice.\n",
    "\n",
    "If you want to check it further, the complete analysis from `analysis.ipynb` with no multicollinearity is in the different branch of the github project (https://github.com/m-dadej/PD_estimation/tree/multicollinearity-impact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
